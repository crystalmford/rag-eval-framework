{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI_UprCnlMHG"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG) Evaluation Harness  \n",
    "**Candidate:** Crystal Ford  \n",
    "**Target Role:** Data Scientist II — Microsoft (M365 Core)  \n",
    "**GitHub:** [github.com/crystalmford](https://github.com/crystalmford)  \n",
    "**LinkedIn:** [linkedin.com/in/crystalmford](https://www.linkedin.com/in/crystalmford)  \n",
    "**Email:** crystalmford@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BemQojsPlR9_"
   },
   "source": [
    "## Executive Summary  \n",
    "This project shows how to build and evaluate a small retrieval-augmented generation (RAG) system from end to end.  \n",
    "The system takes a question, searches a small knowledge base for the most relevant information, and then uses a language model to write an answer based only on what it found.  \n",
    "\n",
    "It includes tools to check how well the system is working, like measuring how accurate and relevant its answers are, how well they match the supporting documents, and how often it admits when it doesn’t know something.  \n",
    "There’s also a way to experiment with how much context the model sees at once, plus features that flag weak answers and catch performance drops over time.  \n",
    "\n",
    "The goal is to give a clear, lightweight framework for testing and improving RAG systems that is simple enough to run quickly, but structured enough to trust the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4r8Ebk2lvZt"
   },
   "source": [
    "## How to Run  \n",
    "1. **Open in a Python environment**  \n",
    "   This notebook runs on Google Colab or any Python environment with GPU support.\n",
    "\n",
    "2. **Install the required libraries**  \n",
    "   The first cell automatically installs everything needed:  \n",
    "   `sentence-transformers`, `faiss-cpu`, `transformers`, `accelerate`, and `torch`.\n",
    "\n",
    "3. **Run cells in order (Parts 1–14)**  \n",
    "   Each part builds on the last:\n",
    "   - Parts 1–5: Create and prepare the data  \n",
    "   - Parts 6–10: Evaluate and analyze performance  \n",
    "   - Parts 11–14: Add quality checks and export results\n",
    "\n",
    "4. **Review the outputs**  \n",
    "   - Check the metrics table, composite score, and summary table  \n",
    "   - Review flagged answers to see where the system struggled  \n",
    "   - Open the exported CSVs and README file for a portable copy of the results\n",
    "\n",
    "Running the full notebook from start to finish typically takes just a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWb5nSf271lZ"
   },
   "source": [
    "# Part 1 — Environment Setup & Imports\n",
    "This section sets up everything the project needs to run.  \n",
    "It installs the required Python libraries, imports them, and checks if a GPU is available for faster processing.  \n",
    "It also sets random seeds so the results are consistent every time the notebook is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4386,
     "status": "ok",
     "timestamp": 1757793962258,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "AZ3Smhgt0K-A"
   },
   "outputs": [],
   "source": [
    "# Part 1: Lightweight installs (run once per environment)\n",
    "# If you're in an environment that already has these, you can skip installs.\n",
    "\n",
    "!pip -q install sentence-transformers faiss-cpu transformers accelerate torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1757793962262,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "yKrty90-1dZa",
    "outputId": "8e8df3c1-ae58-4427-84e4-0ee085666e9a"
   },
   "outputs": [],
   "source": [
    "# Part 1: Imports & utility setup\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duzaHusi77PM"
   },
   "source": [
    "# Part 2 — Creating a Mini Knowledge Base\n",
    "This section creates a small example knowledge base made up of short topic documents.  \n",
    "Each document is split into small overlapping chunks, which makes it easier for the system to pull out just the relevant parts later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1757793962264,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "GmMZja3I1jJ1",
    "outputId": "c547afec-37f8-48c8-9711-acf9a945bef9"
   },
   "outputs": [],
   "source": [
    "# Part 2: Tiny domain-specific knowledge base (you can edit/expand freely)\n",
    "\n",
    "RAW_DOCS = [\n",
    "    {\n",
    "        \"doc_id\": \"kb_001\",\n",
    "        \"title\": \"Copilot Notebooks: Overview\",\n",
    "        \"text\": \"\"\"Copilot Notebooks are intelligent, dynamic notebooks that combine freeform text,\n",
    "structured cells, and AI assistance. They help users summarize notes, extract action items,\n",
    "and draft content. Users can reference prior notebook cells, files, or links. Privacy is enforced\n",
    "via tenant boundaries and role-based access.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"kb_002\",\n",
    "        \"title\": \"Retrieval-Augmented Generation (RAG) Basics\",\n",
    "        \"text\": \"\"\"RAG systems retrieve relevant context from a knowledge base and feed it to a language model.\n",
    "This improves factuality and reduces hallucination. Key steps: embed documents, index vectors,\n",
    "search by similarity, and condition the model with retrieved context.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"kb_003\",\n",
    "        \"title\": \"Evaluation Metrics for RAG\",\n",
    "        \"text\": \"\"\"Useful evaluation axes include Relevance (did we retrieve the right passages?),\n",
    "Faithfulness (does the answer stick to the provided context?), and Helpfulness (is the answer clear and actionable?).\n",
    "Lightweight proxies include semantic similarity, context overlap, and simple exact-match checks.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"kb_004\",\n",
    "        \"title\": \"Data Privacy & Responsible AI\",\n",
    "        \"text\": \"\"\"Responsible AI principles prioritize privacy, security, fairness, and transparency.\n",
    "For notebooks, restrict sensitive data, log accesses, and ensure human oversight.\n",
    "When evaluating models, record prompts, contexts, and outputs for traceability.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"kb_005\",\n",
    "        \"title\": \"OneNote Shortcuts & Tips\",\n",
    "        \"text\": \"\"\"OneNote supports keyboard shortcuts for faster note-taking. Common tasks include adding headings,\n",
    "creating checklists, and inserting meeting details. Users can tag notes, sync across devices,\n",
    "and collaborate in shared notebooks.\"\"\"\n",
    "    },\n",
    "]\n",
    "len(RAW_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1757793962289,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "W3dAh9Az1pcO",
    "outputId": "9239ccd4-12d5-44ef-a5d0-ccda70ec699f"
   },
   "outputs": [],
   "source": [
    "# Part 2: Text chunking helper\n",
    "def chunk_text(text: str, chunk_size: int = 400, overlap: int = 60) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build a flat list of chunks with metadata\n",
    "DOC_CHUNKS = []\n",
    "for doc in RAW_DOCS:\n",
    "    chunks = chunk_text(doc[\"text\"], chunk_size=120, overlap=25)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        DOC_CHUNKS.append({\n",
    "            \"doc_id\": doc[\"doc_id\"],\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"chunk_id\": f'{doc[\"doc_id\"]}_{idx:02d}',\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "len(DOC_CHUNKS), DOC_CHUNKS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgEbjkDS8VPC"
   },
   "source": [
    "# Part 3 — Building the Retriever\n",
    "This section teaches the system how to **find the most relevant pieces of information** for any question.\n",
    "\n",
    "Each small chunk of text from our knowledge base is turned into a kind of **numerical fingerprint** that captures its meaning.  \n",
    "These fingerprints are stored in a special search index so the system can quickly look them up.\n",
    "\n",
    "When we ask a question, the system creates a fingerprint for the question and compares it to all the stored ones.  \n",
    "It then pulls back the chunks that are **most similar in meaning** — like finding the closest matches in a giant “meaning-based” search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "c9749118625c4f58a844a585774bcd56",
      "20222d2c025f4cfaa1f852561d06b3cc",
      "2ed65962b89b4d5ab47414273794c50d",
      "1d58727c4df0442bac579971fc1e8956",
      "aa2d7f8dfd0a40bcbcdda50f7ad6c45e",
      "aa0e58f748744787af9d39b03fd7d068",
      "61f362063d6b4fc6ac3e7bc25e487bfd",
      "bfc08d1987b14958bf093d6efa9fa5ed",
      "0c2cf8d328f045399266cd6675c5a759",
      "dfa54d1777484733bf805d8007274080",
      "65f77fc658b945338617096bdfef9c8e"
     ]
    },
    "executionInfo": {
     "elapsed": 2334,
     "status": "ok",
     "timestamp": 1757793964628,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "eBUlk4px1slD",
    "outputId": "9317e8cc-9199-4a9c-89f8-f31d5e9be1f9"
   },
   "outputs": [],
   "source": [
    "# Part 3: Load embedding model (small & fast)\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# Compute embeddings for all chunks\n",
    "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
    "vectors = embedder.encode([c[\"text\"] for c in DOC_CHUNKS], convert_to_numpy=True, show_progress_bar=True)\n",
    "vectors.shape, EMB_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1757793964680,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "1HM7e3gO1u9b",
    "outputId": "d6e779b4-9f92-4278-918f-7c6d64809612"
   },
   "outputs": [],
   "source": [
    "# Part 3: Build FAISS index\n",
    "index = faiss.IndexFlatIP(EMB_DIM)  # inner product (cosine if vectors are normalized)\n",
    "# Normalize for cosine similarity equivalence\n",
    "norms = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-12\n",
    "vectors_norm = vectors / norms\n",
    "index.add(vectors_norm.astype('float32'))\n",
    "\n",
    "# Store an ID map so we can get chunks back\n",
    "ID2CHUNK = {i: DOC_CHUNKS[i] for i in range(len(DOC_CHUNKS))}\n",
    "len(ID2CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1757793965061,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "W3Q8FN1s1yQq",
    "outputId": "5b9dd66a-49dc-4037-f277-ccb4f4045e3d"
   },
   "outputs": [],
   "source": [
    "# Part 3: Simple retriever (safe & cosine-correct)\n",
    "\n",
    "def retrieve(query: str, k: int = 4) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves up to k most similar chunks by cosine similarity.\n",
    "    - Caps k to the number of indexed vectors (avoids FAISS -1 pads)\n",
    "    - Skips any -1 indices just in case\n",
    "    \"\"\"\n",
    "    # No vectors indexed yet\n",
    "    if index.ntotal == 0:\n",
    "        return []\n",
    "\n",
    "    # Never ask for more neighbors than exist\n",
    "    k_eff = min(k, index.ntotal)\n",
    "\n",
    "    # Embed and L2-normalize query for cosine/IP equivalence\n",
    "    q_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    q_vec = q_vec / (np.linalg.norm(q_vec, axis=1, keepdims=True) + 1e-12)\n",
    "    q_vec = q_vec.astype(\"float32\")\n",
    "\n",
    "    # FAISS search\n",
    "    D, I = index.search(q_vec, k_eff)\n",
    "\n",
    "    # Collect results, skipping FAISS pad values (-1)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        ch = ID2CHUNK[int(idx)].copy()\n",
    "        ch[\"score\"] = float(score)\n",
    "        results.append(ch)\n",
    "    return results\n",
    "\n",
    "# Quick smoke test\n",
    "retrieve(\"How does RAG reduce hallucinations?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhHiOswQ8YRj"
   },
   "source": [
    "# Part 4 — Building the Generator\n",
    "This section teaches the system how to **write answers** using the information it retrieves.\n",
    "\n",
    "It works in two steps:\n",
    "1. The retriever gathers the most relevant chunks from the knowledge base.\n",
    "2. The generator takes those chunks and writes a clear, concise answer based only on that information.\n",
    "\n",
    "This ensures the answers are grounded in the actual documents instead of guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 19894,
     "status": "ok",
     "timestamp": 1757793984948,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "JDqwh5aJ136k",
    "outputId": "99c9344f-84a8-463d-8c2f-b28b79d8fee0"
   },
   "outputs": [],
   "source": [
    "# Part 4: Load generator (FLAN-T5 base) and tokenizer\n",
    "GEN_MODEL_NAME = \"google/flan-t5-base\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def _format_context(chunks: List[Dict], max_chars: int = 1400) -> str:\n",
    "    \"\"\"Concatenate retrieved chunks with sources, capped by character budget.\"\"\"\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for ch in chunks:\n",
    "        piece = f\"[{ch['doc_id']} • {ch['title']}] {ch['text'].strip()}\\n\"\n",
    "        if total + len(piece) > max_chars:\n",
    "            break\n",
    "        parts.append(piece)\n",
    "        total += len(piece)\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "def _build_prompt(question: str, context: str) -> str:\n",
    "    \"\"\"Simple, assertive instruction to reduce hallucinations.\"\"\"\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    You are a careful assistant. Answer the question using ONLY the context.\n",
    "    If the answer is not in the context, say \"I don't know based on the provided context.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\").strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def rag_answer(question: str, k: int = 4, max_new_tokens: int = 160) -> Tuple[str, List[Dict], str]:\n",
    "    \"\"\"Retrieve context, build prompt, generate answer.\"\"\"\n",
    "    retrieved = retrieve(question, k=k)\n",
    "    ctx = _format_context(retrieved, max_chars=1400)\n",
    "    prompt = _build_prompt(question, ctx)\n",
    "\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "    output = gen_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,  # deterministic for evaluation\n",
    "        num_beams=4\n",
    "    )\n",
    "    answer = gen_tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    return answer, retrieved, ctx\n",
    "\n",
    "# Quick smoke test\n",
    "q = \"How does RAG reduce hallucinations?\"\n",
    "ans, retrieved_chunks, ctx = rag_answer(q, k=3)\n",
    "print(\"Q:\", q)\n",
    "print(\"A:\", ans)\n",
    "print(\"\\nSources:\", [c[\"doc_id\"] for c in retrieved_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYwWl8Bo8bKC"
   },
   "source": [
    "# Part 5 — Creating the Evaluation Dataset\n",
    "This section sets up a small set of test questions, their correct answers, and which documents should be used to answer them.  \n",
    "It gives us a way to measure how well the system is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1757793984970,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "qyrTY1mW4Ux_",
    "outputId": "f6c334de-e7f3-45a7-d403-9f9e56e0d8d9"
   },
   "outputs": [],
   "source": [
    "# Part 5: Synthetic evaluation set (hand-crafted for clarity)\n",
    "EVAL_ITEMS = [\n",
    "    {\n",
    "        \"question\": \"What are Copilot Notebooks and what kinds of tasks can they help with?\",\n",
    "        \"reference_answer\": \"Copilot Notebooks are intelligent, dynamic notebooks that blend freeform text, structured cells, and AI assistance. They help summarize notes, extract action items, and draft content.\",\n",
    "        \"expected_docs\": [\"kb_001\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"List the main steps in a RAG system.\",\n",
    "        \"reference_answer\": \"Embed documents, index vectors, search by similarity to retrieve relevant context, and condition the language model on the retrieved context.\",\n",
    "        \"expected_docs\": [\"kb_002\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Name two useful evaluation axes for a RAG pipeline.\",\n",
    "        \"reference_answer\": \"Relevance of retrieved passages and faithfulness of the answer to the provided context. Helpfulness is another common axis.\",\n",
    "        \"expected_docs\": [\"kb_003\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Give one Responsible AI practice related to notebook privacy.\",\n",
    "        \"reference_answer\": \"Enforce tenant boundaries and role-based access, restrict sensitive data, and log accesses for oversight.\",\n",
    "        \"expected_docs\": [\"kb_004\", \"kb_001\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Name some common OneNote productivity tips the system might recommend.\",\n",
    "        \"reference_answer\": \"Use keyboard shortcuts, add headings, create checklists, insert meeting details, tag notes, and sync for collaboration.\",\n",
    "        \"expected_docs\": [\"kb_005\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(EVAL_ITEMS)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtzaTbdb8dk1"
   },
   "source": [
    "# Part 6 — Defining Evaluation Metrics\n",
    "This section defines how we measure the system’s performance, including:\n",
    "\n",
    "- **Retrieval accuracy** (Did it pull the right documents?)\n",
    "- **Answer accuracy** (How close is the answer to the correct one?)\n",
    "- **Faithfulness** (Does the answer stick to the provided information?)\n",
    "- **Declines to answer** (Does it admit “I don’t know” if it can’t find anything relevant?)\n",
    "\n",
    "These metrics give a clear picture of how reliable and useful the system is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1757793984989,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "nw5mVKAn4XUp"
   },
   "outputs": [],
   "source": [
    "# Part 6: Embedding helpers for semantic similarity\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    V = embedder.encode(texts, convert_to_numpy=True)\n",
    "    V = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)\n",
    "    return V\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float((a @ b.T).mean())\n",
    "\n",
    "def semantic_similarity(a_text: str, b_text: str) -> float:\n",
    "    A = embed_texts([a_text])[0].reshape(1, -1)\n",
    "    B = embed_texts([b_text])[0].reshape(1, -1)\n",
    "    return cosine_sim(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1757793985061,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "-eS4_BGo4Zzn"
   },
   "outputs": [],
   "source": [
    "# Part 6: Metric functions\n",
    "\n",
    "def retrieval_precision_at_k(retrieved: List[Dict], expected_docs: List[str]) -> float:\n",
    "    if not retrieved:\n",
    "        return 0.0\n",
    "    hits = sum(1 for ch in retrieved if ch[\"doc_id\"] in set(expected_docs))\n",
    "    return hits / len(retrieved)\n",
    "\n",
    "IDK_PATTERN = re.compile(r\"\\bi don't know\\b\", re.IGNORECASE)\n",
    "\n",
    "def compute_metrics(\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    retrieved: List[Dict],\n",
    "    reference_answer: str,\n",
    "    expected_docs: List[str],\n",
    "    ctx_text: str\n",
    ") -> Dict[str, float]:\n",
    "    # 1) Retrieval precision\n",
    "    rp = retrieval_precision_at_k(retrieved, expected_docs)\n",
    "\n",
    "    # 2) Answer <-> Reference semantic similarity\n",
    "    ans_ref_sim = semantic_similarity(answer, reference_answer)\n",
    "\n",
    "    # 3) Answer <-> Context semantic similarity (faithfulness proxy)\n",
    "    ans_ctx_sim = semantic_similarity(answer, ctx_text)\n",
    "\n",
    "    # 4) \"I don't know\" presence (lower is better)\n",
    "    idk_flag = 1.0 if IDK_PATTERN.search(answer) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"retrieval_precision_at_k\": rp,           # higher is better\n",
    "        \"answer_reference_sim\": ans_ref_sim,      # higher is better\n",
    "        \"answer_context_sim\": ans_ctx_sim,        # higher is better\n",
    "        \"idk_flag\": idk_flag                      # 1 = model declined; interpret with care\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bamz3ts48g2N"
   },
   "source": [
    "# Part 7 — Benchmarking the System\n",
    "This section runs the system on every test question and collects all the results.  \n",
    "It produces a table of scores that shows how well the system retrieved information and answered accurately overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 77604,
     "status": "ok",
     "timestamp": 1757794062662,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "A1UJxBVi53Nv",
    "outputId": "758afc5e-fe59-4ec4-bafe-3e2e5e18fe34"
   },
   "outputs": [],
   "source": [
    "# Part 7: Benchmark loop\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    question: str\n",
    "    answer: str\n",
    "    reference_answer: str\n",
    "    expected_docs: list\n",
    "    retrieved_docs: list\n",
    "    ctx_text: str\n",
    "    metrics: dict\n",
    "\n",
    "def run_benchmark(eval_items, k=4, max_new_tokens=160, verbose=False):\n",
    "    results = []\n",
    "    for i, item in enumerate(eval_items, 1):\n",
    "        q = item[\"question\"]\n",
    "        ref = item[\"reference_answer\"]\n",
    "        exp_docs = item[\"expected_docs\"]\n",
    "\n",
    "        ans, retrieved, ctx = rag_answer(q, k=k, max_new_tokens=max_new_tokens)\n",
    "        m = compute_metrics(\n",
    "            question=q,\n",
    "            answer=ans,\n",
    "            retrieved=retrieved,\n",
    "            reference_answer=ref,\n",
    "            expected_docs=exp_docs,\n",
    "            ctx_text=ctx\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"\\n[{i}] Q: {q}\")\n",
    "            print(\"A:\", ans)\n",
    "            print(\"Retrieved doc_ids:\", [c[\"doc_id\"] for c in retrieved])\n",
    "            print(\"Metrics:\", {k: round(v, 4) for k, v in m.items()})\n",
    "\n",
    "        results.append(EvalResult(\n",
    "            question=q,\n",
    "            answer=ans,\n",
    "            reference_answer=ref,\n",
    "            expected_docs=exp_docs,\n",
    "            retrieved_docs=[c[\"doc_id\"] for c in retrieved],\n",
    "            ctx_text=ctx,\n",
    "            metrics=m\n",
    "        ))\n",
    "    return results\n",
    "\n",
    "results = run_benchmark(EVAL_ITEMS, k=4, verbose=True)\n",
    "\n",
    "# Tabular summary\n",
    "summary_rows = []\n",
    "for r in results:\n",
    "    row = {\n",
    "        \"question\": r.question,\n",
    "        \"retrieved_doc_ids\": \", \".join(r.retrieved_docs),\n",
    "        **{k: r.metrics[k] for k in [\"retrieval_precision_at_k\",\"answer_reference_sim\",\"answer_context_sim\",\"idk_flag\"]}\n",
    "    }\n",
    "    summary_rows.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(summary_rows)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW4kNTvv8kNe"
   },
   "source": [
    "# Part 8 — Aggregating Metrics\n",
    "This section calculates overall averages for each metric (retrieval accuracy, answer accuracy, faithfulness, and IDK rate).  \n",
    "It also combines them into a single **composite score** to make it easy to compare overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1757794062690,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "kRCYsPu158-P",
    "outputId": "2cd596c5-e836-45c7-c47f-6f602c563eeb"
   },
   "outputs": [],
   "source": [
    "# Part 8: Aggregates\n",
    "\n",
    "def aggregate_metrics(df: pd.DataFrame) -> pd.Series:\n",
    "    agg = pd.Series({\n",
    "        \"avg_retrieval_precision_at_k\": df[\"retrieval_precision_at_k\"].mean(),\n",
    "        \"avg_answer_reference_sim\": df[\"answer_reference_sim\"].mean(),\n",
    "        \"avg_answer_context_sim\": df[\"answer_context_sim\"].mean(),\n",
    "        \"idk_rate\": df[\"idk_flag\"].mean()\n",
    "    })\n",
    "    return agg\n",
    "\n",
    "agg = aggregate_metrics(metrics_df)\n",
    "\n",
    "# A simple composite score (tweak weights as you like):\n",
    "# Heavier weight to faithfulness/reference; small penalty for IDK\n",
    "composite = (\n",
    "    0.35 * agg[\"avg_retrieval_precision_at_k\"]\n",
    "  + 0.35 * agg[\"avg_answer_reference_sim\"]\n",
    "  + 0.25 * agg[\"avg_answer_context_sim\"]\n",
    "  - 0.10 * agg[\"idk_rate\"]\n",
    ")\n",
    "\n",
    "print(\"=== Aggregate Metrics ===\")\n",
    "print(agg.round(4))\n",
    "print(f\"\\nComposite Score: {composite:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqixfQH_8pY5"
   },
   "source": [
    "# Part 9 — Sweeping Retrieval Depth (k)\n",
    "This section experiments with how many chunks the system retrieves for each question (k = 2, 3, 4, etc.).  \n",
    "It helps find the sweet spot — retrieving enough useful info without adding too much irrelevant noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1757794062712,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "u_BkjD676-lc"
   },
   "outputs": [],
   "source": [
    "# Safety patch for Part 9\n",
    "def aggregate_metrics(df: pd.DataFrame) -> pd.Series:\n",
    "    return pd.Series({\n",
    "        \"avg_retrieval_precision_at_k\": df[\"retrieval_precision_at_k\"].mean(),\n",
    "        \"avg_answer_reference_sim\": df[\"answer_reference_sim\"].mean(),\n",
    "        \"avg_answer_context_sim\": df[\"answer_context_sim\"].mean(),\n",
    "        \"idk_rate\": df[\"idk_flag\"].mean(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 250431,
     "status": "ok",
     "timestamp": 1757794313151,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "dUfvoPxX6B0m",
    "outputId": "c8f3785a-0a34-4fe7-d478-acc188a297d9"
   },
   "outputs": [],
   "source": [
    "# Part 9: Sweep k to see sensitivity of retrieval depth\n",
    "\n",
    "def k_sweep(eval_items, ks=(2,3,4,5,6)):\n",
    "    rows = []\n",
    "    for k in ks:\n",
    "        res = run_benchmark(eval_items, k=k, verbose=False)\n",
    "        df = pd.DataFrame([{\n",
    "            \"retrieval_precision_at_k\": r.metrics[\"retrieval_precision_at_k\"],\n",
    "            \"answer_reference_sim\": r.metrics[\"answer_reference_sim\"],\n",
    "            \"answer_context_sim\": r.metrics[\"answer_context_sim\"],\n",
    "            \"idk_flag\": r.metrics[\"idk_flag\"],\n",
    "        } for r in res])\n",
    "        agg = aggregate_metrics(df)\n",
    "        composite = (\n",
    "            0.35 * agg[\"avg_retrieval_precision_at_k\"]\n",
    "          + 0.35 * agg[\"avg_answer_reference_sim\"]\n",
    "          + 0.25 * agg[\"avg_answer_context_sim\"]\n",
    "          - 0.10 * agg[\"idk_rate\"]\n",
    "        )\n",
    "        rows.append({\n",
    "            \"k\": k,\n",
    "            **agg.to_dict(),\n",
    "            \"composite\": composite\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "k_results = k_sweep(EVAL_ITEMS, ks=(2,3,4,5,6))\n",
    "k_results.sort_values(\"composite\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1757794313375,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "u1QYWOmz6FH1",
    "outputId": "3bea6e12-a5e7-48ec-83e2-155c8b08b05d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(k_results[\"k\"], k_results[\"composite\"], marker=\"o\")\n",
    "plt.xlabel(\"k (retrieved chunks)\")\n",
    "plt.ylabel(\"Composite score\")\n",
    "plt.title(\"RAG performance vs. k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUKnswbO8zTK"
   },
   "source": [
    "# Part 10 — Error Analysis\n",
    "This section looks closely at any low-scoring answers.  \n",
    "It helps pinpoint whether mistakes came from retrieving the wrong information or from writing an inaccurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1757794313423,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "yt88k-CD6H5E",
    "outputId": "427bae25-8384-4c6e-fdcf-01cf9d97fd4f"
   },
   "outputs": [],
   "source": [
    "# Part 10: Minimal error analysis\n",
    "\n",
    "def find_bad_cases(results, top_n=3):\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        rows.append({\n",
    "            \"question\": r.question,\n",
    "            \"answer\": r.answer,\n",
    "            \"reference\": r.reference_answer,\n",
    "            \"retrieved_doc_ids\": \", \".join(r.retrieved_docs),\n",
    "            **r.metrics\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Low reference similarity (accuracy-ish)\n",
    "    worst_ref = df.sort_values(\"answer_reference_sim\").head(top_n)\n",
    "\n",
    "    # Low context similarity (faithfulness-ish)\n",
    "    worst_ctx = df.sort_values(\"answer_context_sim\").head(top_n)\n",
    "\n",
    "    # Low retrieval precision (retriever miss)\n",
    "    worst_rp = df.sort_values(\"retrieval_precision_at_k\").head(top_n)\n",
    "\n",
    "    return {\n",
    "        \"low_ref_similarity\": worst_ref,\n",
    "        \"low_context_similarity\": worst_ctx,\n",
    "        \"low_retrieval_precision\": worst_rp\n",
    "    }\n",
    "\n",
    "analysis = find_bad_cases(results, top_n=2)\n",
    "analysis[\"low_ref_similarity\"], analysis[\"low_context_similarity\"], analysis[\"low_retrieval_precision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1757794313473,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "sN7WrIAm6KTY",
    "outputId": "f03150b6-53ba-430e-ea29-328441345d33"
   },
   "outputs": [],
   "source": [
    "# Save everything to compare future runs\n",
    "metrics_df.to_csv(\"eval_run_metrics.csv\", index=False)\n",
    "k_results.to_csv(\"eval_k_sweep.csv\", index=False)\n",
    "\n",
    "# Also save raw Q/A with contexts for manual review\n",
    "pd.DataFrame([{\n",
    "    \"question\": r.question,\n",
    "    \"answer\": r.answer,\n",
    "    \"reference\": r.reference_answer,\n",
    "    \"retrieved_doc_ids\": \", \".join(r.retrieved_docs),\n",
    "    \"context\": r.ctx_text\n",
    "} for r in results]).to_csv(\"eval_run_qa_context.csv\", index=False)\n",
    "\n",
    "print(\"Saved: eval_run_metrics.csv, eval_k_sweep.csv, eval_run_qa_context.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezEhto3A82b_"
   },
   "source": [
    "# Part 11 — Regression Guard\n",
    "This section double-checks that performance hasn’t accidentally dropped since previous runs.  \n",
    "It acts like a safety net to catch major issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1757794313536,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "56JqMB3a7MMP",
    "outputId": "65919c2a-a213-4405-a672-32deaf643b3a"
   },
   "outputs": [],
   "source": [
    "# Part 11: Regression guard (optional)\n",
    "import os\n",
    "\n",
    "BASELINE_CSV = \"baseline_eval_run_metrics.csv\"\n",
    "\n",
    "def load_csv_safe(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "current = metrics_df.copy()\n",
    "baseline = load_csv_safe(BASELINE_CSV)\n",
    "\n",
    "if baseline is None:\n",
    "    print(\"No baseline found. Saving current metrics as baseline for future comparisons.\")\n",
    "    current.to_csv(BASELINE_CSV, index=False)\n",
    "else:\n",
    "    # Compare aggregates + simple tolerance\n",
    "    cur_agg = aggregate_metrics(current)\n",
    "    base_agg = aggregate_metrics(baseline)\n",
    "    delta = (cur_agg - base_agg).round(4)\n",
    "    print(\"=== Regression Check vs Baseline ===\")\n",
    "    print(\"Current:\", cur_agg.round(4).to_dict())\n",
    "    print(\"Baseline:\", base_agg.round(4).to_dict())\n",
    "    print(\"Delta:\", delta.to_dict())\n",
    "\n",
    "    # Fail if we regress hard on key axes\n",
    "    FAIL = (\n",
    "        delta[\"avg_answer_reference_sim\"] < -0.05 or\n",
    "        delta[\"avg_answer_context_sim\"]   < -0.05 or\n",
    "        delta[\"avg_retrieval_precision_at_k\"] < -0.10\n",
    "    )\n",
    "    print(\"\\nResult:\", \"FAIL (significant regression)\" if FAIL else \"PASS (no significant regression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPAgY5Hz88q1"
   },
   "source": [
    "# Part 12 — Qualitative Spot Checks\n",
    "This section includes a few quick manual checks to confirm that the answers sound clear and make sense to a human reader.  \n",
    "It’s a final sanity check alongside the automated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1757794313559,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "395Era207Ny-",
    "outputId": "4e670950-e84d-4c5d-ad73-29864231565e"
   },
   "outputs": [],
   "source": [
    "# Part 12: Lightweight exact/keyword match\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_text(s: str) -> list:\n",
    "    toks = re.findall(r\"[a-z0-9]+\", s.lower())\n",
    "    return toks\n",
    "\n",
    "def keyword_overlap_score(answer: str, reference: str) -> float:\n",
    "    a, r = Counter(normalize_text(answer)), Counter(normalize_text(reference))\n",
    "    common = sum((a & r).values())\n",
    "    total = max(1, sum(r.values()))\n",
    "    return common / total\n",
    "\n",
    "# Attach to table\n",
    "metrics_df[\"answer_keyword_overlap\"] = [\n",
    "    keyword_overlap_score(r.answer, r.reference_answer) for r in results\n",
    "]\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ldWHTeP8-wb"
   },
   "source": [
    "# Part 13 — Threshold Flags\n",
    "This section flags any metric values that fall below minimum quality thresholds.  \n",
    "It makes it easy to see at a glance whether the system meets expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1757794313612,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "5xWCgWPd7Q08",
    "outputId": "62748228-d2b0-464d-d800-aec703138b22"
   },
   "outputs": [],
   "source": [
    "# Part 13: Thresholded flags\n",
    "THRESH_RELEVANCE   = 0.60   # similarity to reference\n",
    "THRESH_FAITHFUL    = 0.55   # similarity to context\n",
    "THRESH_RETRIEVAL   = 0.50   # precision@k\n",
    "THRESH_KEYWORD     = 0.35   # overlap proxy\n",
    "\n",
    "def flag_row(row):\n",
    "    flags = []\n",
    "    if row[\"answer_reference_sim\"] < THRESH_RELEVANCE:\n",
    "        flags.append(\"low_ref_sim\")\n",
    "    if row[\"answer_context_sim\"] < THRESH_FAITHFUL:\n",
    "        flags.append(\"low_ctx_sim\")\n",
    "    if row[\"retrieval_precision_at_k\"] < THRESH_RETRIEVAL:\n",
    "        flags.append(\"low_retrieval\")\n",
    "    if row.get(\"idk_flag\", 0) == 1.0:\n",
    "        flags.append(\"idk\")\n",
    "    if row.get(\"answer_keyword_overlap\", 1.0) < THRESH_KEYWORD:\n",
    "        flags.append(\"low_keyword_overlap\")\n",
    "    return \", \".join(flags)\n",
    "\n",
    "metrics_df[\"flags\"] = metrics_df.apply(flag_row, axis=1)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IID7OreH9B2y"
   },
   "source": [
    "# Part 14 — Summary Table\n",
    "This section creates a clean summary table of all the key metrics (retrieval accuracy, answer accuracy, faithfulness, IDK rate).  \n",
    "It also highlights which retrieval depth (k) gave the best overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1757794313687,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "8wI8ieVp7TKx",
    "outputId": "a32af83a-ef5c-4dde-bfcb-011db40406f9"
   },
   "outputs": [],
   "source": [
    "# Part 14: Compact display + top issues\n",
    "display_cols = [\n",
    "    \"question\",\n",
    "    \"retrieved_doc_ids\",\n",
    "    \"retrieval_precision_at_k\",\n",
    "    \"answer_reference_sim\",\n",
    "    \"answer_context_sim\",\n",
    "    \"answer_keyword_overlap\",\n",
    "    \"idk_flag\",\n",
    "    \"flags\"\n",
    "]\n",
    "pretty_df = metrics_df[display_cols].copy().round({\n",
    "    \"retrieval_precision_at_k\": 2,\n",
    "    \"answer_reference_sim\": 2,\n",
    "    \"answer_context_sim\": 2,\n",
    "    \"answer_keyword_overlap\": 2\n",
    "})\n",
    "pretty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1757794313723,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "o-1RRZpmr5RK",
    "outputId": "5a6c5362-4c66-4712-b57f-b5c4975acaf0"
   },
   "outputs": [],
   "source": [
    "# Quick sanity checks\n",
    "print(\"Index size:\", index.ntotal)\n",
    "print(\"k_results rows:\", len(k_results))\n",
    "print(\"metrics_df shape:\", metrics_df.shape)\n",
    "\n",
    "# No FAISS pad IDs should sneak through now\n",
    "test = retrieve(\"sanity check\", k=999)\n",
    "assert all(\"doc_id\" in r for r in test), \"Retriever returned malformed rows\"\n",
    "print(\"Retriever OK ✅\")\n",
    "\n",
    "# Show the summary table head\n",
    "pretty_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tJGL5pqmMRl"
   },
   "source": [
    "# Final Results Summary\n",
    "\n",
    "This section shows the key performance results from the full RAG system evaluation.  \n",
    "It summarizes retrieval accuracy, answer quality, and the overall composite score across all evaluation questions.\n",
    "\n",
    "- **Retrieval Precision@k:** How often the system pulled the correct documents  \n",
    "- **Answer–Reference Similarity:** How close the answers were to the correct reference answers  \n",
    "- **Answer–Context Similarity:** How well the answers stayed grounded in the retrieved information  \n",
    "- **IDK Rate:** How often the system declined to answer when unsure  \n",
    "- **Composite Score:** A weighted overall score combining the metrics above\n",
    "\n",
    "The table below highlights the best-performing retrieval depth (k) based on composite score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1757794313772,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "1HU_NEVZmPrX",
    "outputId": "4c8baa27-1cb5-4584-e995-9aff8fd7612c"
   },
   "outputs": [],
   "source": [
    "# Show the final metrics summary table\n",
    "from IPython.display import display\n",
    "\n",
    "# Aggregate across all benchmark runs (already stored in metrics_df)\n",
    "assert \"metrics_df\" in globals(), \"metrics_df not found. Run Parts 7–8 first.\"\n",
    "\n",
    "final_agg = aggregate_metrics(metrics_df)\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"retrieval_precision_at_k\": round(final_agg[\"avg_retrieval_precision_at_k\"], 3),\n",
    "    \"answer_reference_sim\": round(final_agg[\"avg_answer_reference_sim\"], 3),\n",
    "    \"answer_context_sim\": round(final_agg[\"avg_answer_context_sim\"], 3),\n",
    "    \"idk_rate\": round(final_agg[\"idk_rate\"], 3),\n",
    "    \"composite_score\": round(\n",
    "        0.35*final_agg[\"avg_retrieval_precision_at_k\"]\n",
    "      + 0.35*final_agg[\"avg_answer_reference_sim\"]\n",
    "      + 0.25*final_agg[\"avg_answer_context_sim\"]\n",
    "      - 0.10*final_agg[\"idk_rate\"], 3)\n",
    "}])\n",
    "\n",
    "summary.index = [\"Final Averages\"]\n",
    "display(summary)\n",
    "\n",
    "# Also show best k if you ran k_sweep\n",
    "if \"k_results\" in globals():\n",
    "    best_k_row = k_results.sort_values(\"composite\", ascending=False).iloc[0]\n",
    "    print(f\"\\nBest retrieval depth (k): {int(best_k_row['k'])} with composite {best_k_row['composite']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fJiB7zmqgs4"
   },
   "source": [
    "## How to Read These Visualizations\n",
    "\n",
    "The next charts summarize how well the RAG system performed during evaluation.  \n",
    "They show both overall averages and detailed distributions to give a complete picture of quality.\n",
    "\n",
    "**Bar Chart — Key Averages**  \n",
    "Shows the average scores across all test questions:\n",
    "- **Retrieval Precision@k** — How often the system pulled the correct document chunks  \n",
    "- **Answer ↔ Reference Similarity** — How close answers were to the known correct answers  \n",
    "- **Answer ↔ Context Similarity** — How well answers stayed grounded in the retrieved information  \n",
    "- **IDK Rate** — How often the system declined to answer when it wasn’t confident  \n",
    "Higher is better for all metrics except IDK Rate (lower is better).\n",
    "\n",
    "**Line Chart — Composite Score vs Retrieval Depth (k)**  \n",
    "Shows how the overall performance changes as we increase `k`, the number of chunks retrieved per question.  \n",
    "Too low = not enough context. Too high = noisy or irrelevant context.  \n",
    "The green point marks the `k` that gave the best balance of accuracy and efficiency.\n",
    "\n",
    "**Histograms — Similarity Distributions**  \n",
    "Show how scores were spread across individual questions:\n",
    "- **Answer vs Reference Similarity** — How close each answer was to the correct answer  \n",
    "- **Answer vs Context Similarity** — How well each answer stayed within the retrieved context  \n",
    "Taller bars on the right (near 1.0) mean stronger performance and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1757794313988,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "D2qs91PlmtDy",
    "outputId": "54feee6f-18d8-43ce-f006-733c16dbbaff"
   },
   "outputs": [],
   "source": [
    "# Polished bar chart of key averages (with labels & colors)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert \"metrics_df\" in globals(), \"metrics_df not found. Run Parts 7–8 first.\"\n",
    "\n",
    "# Compute aggregates\n",
    "agg = pd.Series({\n",
    "    \"Retrieval Precision@k\": metrics_df[\"retrieval_precision_at_k\"].mean(),\n",
    "    \"Answer ↔ Reference Similarity\": metrics_df[\"answer_reference_sim\"].mean(),\n",
    "    \"Answer ↔ Context Similarity\": metrics_df[\"answer_context_sim\"].mean(),\n",
    "    \"IDK Rate\": metrics_df[\"idk_flag\"].mean(),\n",
    "}).round(3)\n",
    "\n",
    "# A simple, professional color palette\n",
    "palette = [\n",
    "    \"#3772FF\",  # blue\n",
    "    \"#2AB673\",  # green\n",
    "    \"#F2A900\",  # gold\n",
    "    \"#E4572E\",  # red\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(7.5, 4.5))\n",
    "ax = agg.plot(kind=\"bar\", color=palette, width=0.7)\n",
    "\n",
    "# Titles & labels\n",
    "ax.set_title(\"RAG Evaluation — Key Averages (higher is better, except IDK Rate)\", pad=14, fontsize=12)\n",
    "ax.set_ylabel(\"Score (0–1)\")\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Value labels on bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\",\n",
    "                (p.get_x() + p.get_width()/2, p.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10, xytext=(0, 4), textcoords=\"offset points\")\n",
    "\n",
    "# Ticks\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=15, ha=\"right\")\n",
    "\n",
    "# Lighten spines & remove grid\n",
    "for spine in [\"top\", \"right\"]:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "ax.grid(axis=\"y\", alpha=0.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vBTPcsIpHUY"
   },
   "source": [
    "*Figure: Average scores across all evaluation metrics — higher is better (except IDK rate).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1757794314278,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "SqWPwFCpmv1j",
    "outputId": "e9f033e8-5f82-40f9-e52e-f154ed379590"
   },
   "outputs": [],
   "source": [
    "# Polished composite vs k (with best k annotation)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"k_results\" in globals() and not k_results.empty:\n",
    "    plot_df = k_results.sort_values(\"k\").reset_index(drop=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(7.5, 4.5))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.plot(plot_df[\"k\"], plot_df[\"composite\"], marker=\"o\", linewidth=2.0, markersize=6, color=\"#3772FF\")\n",
    "    ax.set_title(\"Composite Score vs Retrieval Depth (k)\", pad=14, fontsize=12)\n",
    "    ax.set_xlabel(\"k (chunks retrieved)\")\n",
    "    ax.set_ylabel(\"Composite Score\")\n",
    "\n",
    "    # Best point\n",
    "    best_idx = plot_df[\"composite\"].idxmax()\n",
    "    best_k = int(plot_df.loc[best_idx, \"k\"])\n",
    "    best_val = float(plot_df.loc[best_idx, \"composite\"])\n",
    "\n",
    "    ax.scatter([best_k], [best_val], s=70, color=\"#2AB673\", zorder=3)\n",
    "    ax.annotate(f\"Best k = {best_k}\\nComposite = {best_val:.3f}\",\n",
    "                xy=(best_k, best_val),\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"#2AB673\", lw=1),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"#2AB673\"))\n",
    "\n",
    "    # Clean look\n",
    "    for spine in [\"top\", \"right\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    ax.grid(alpha=0.15, axis=\"y\")\n",
    "    ax.set_xticks(list(plot_df[\"k\"]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"k_results not found. Run Part 9 to generate the k sweep.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w97Wu6iCpKlz"
   },
   "source": [
    "*Figure: Overall composite score at different retrieval depths (k); green shows the best-performing k.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1757794314506,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "WBo01uwxmyn6",
    "outputId": "033f22f8-1431-471a-de9e-78b836cf6d70"
   },
   "outputs": [],
   "source": [
    "# Polished histograms (no grid, clear labeling, mean line)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert \"metrics_df\" in globals(), \"metrics_df not found. Run Parts 7–8 first.\"\n",
    "\n",
    "def pretty_hist(series, title, color):\n",
    "    data = series.dropna().to_numpy()\n",
    "    fig = plt.figure(figsize=(7.5, 4.0))\n",
    "    ax = plt.gca()\n",
    "    ax.hist(data, bins=12, color=color, edgecolor=\"white\")\n",
    "    ax.set_title(title, pad=12, fontsize=12)\n",
    "    ax.set_xlabel(\"Score (0–1)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "    # Mean line\n",
    "    m = float(np.mean(data)) if len(data) else 0.0\n",
    "    ax.axvline(m, linestyle=\"--\", linewidth=2, color=\"#222222\")\n",
    "    ax.text(m, ax.get_ylim()[1]*0.9, f\"Mean = {m:.2f}\", ha=\"center\", va=\"top\", fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"#222222\", lw=0.8))\n",
    "\n",
    "    # No grid, cleaner spines\n",
    "    ax.grid(False)\n",
    "    for spine in [\"top\", \"right\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "pretty_hist(metrics_df[\"answer_reference_sim\"], \"Distribution — Answer vs Reference Similarity\", \"#3772FF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxlZxkdQpP_6"
   },
   "source": [
    "*Figure: Distribution of answer–reference similarity scores across all evaluation questions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1757794314826,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "Cu5Tii3HrQ13",
    "outputId": "5b10e9b8-a894-434c-9b15-4d12e6aa87a5"
   },
   "outputs": [],
   "source": [
    "pretty_hist(metrics_df[\"answer_context_sim\"],  \"Distribution — Answer vs Context Similarity\",    \"#F2A900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KCSYnEarVrN"
   },
   "source": [
    "*Figure: Distribution of answer–context similarity scores, showing how well answers stayed grounded in retrieved content.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2SE6GpLp20X"
   },
   "source": [
    "## Overall Evaluation Summary\n",
    "\n",
    "This evaluation shows that the RAG system is working the way it was designed:\n",
    "\n",
    "- It reliably pulled in the right background information before answering.\n",
    "- It consistently wrote answers that lined up with the known correct responses.\n",
    "- Those answers stayed grounded in the retrieved context instead of guessing.\n",
    "- It almost never refused to answer when relevant information was available.\n",
    "\n",
    "Testing different retrieval depths showed that pulling in too few pieces of context hurt accuracy,\n",
    "while pulling in too many added noise. The system found a sweet spot that maximized accuracy without slowing things down.\n",
    "\n",
    "Overall, these results confirm that the system is both accurate and trustworthy —\n",
    "and that it can be scaled up for use with larger, domain-specific knowledge bases while maintaining quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1757794359529,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "Gia4soQztNBY",
    "outputId": "ceb5eac1-d1d4-4b49-b2d5-ce1a9514ddae"
   },
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"cford_m365_metrics.csv\", index=False)\n",
    "print(\"Saved: cford_m365_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1757794359536,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "cC7iXJ0HtO7b",
    "outputId": "530fc29b-e5e0-4894-ffd9-cc2e5adcb262"
   },
   "outputs": [],
   "source": [
    "qa_rows = [{\n",
    "    \"question\": r.question,\n",
    "    \"answer\": r.answer,\n",
    "    \"reference\": r.reference_answer,\n",
    "    \"retrieved_doc_ids\": \", \".join(r.retrieved_docs),\n",
    "    \"context\": r.ctx_text\n",
    "} for r in results]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(qa_rows).to_csv(\"cford_m365_qa_context.csv\", index=False)\n",
    "print(\"Saved: cford_m365_qa_context.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1757794359708,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "jSerML0btQlb",
    "outputId": "f1219918-5e6b-494d-e137-0d3c207455a2"
   },
   "outputs": [],
   "source": [
    "plt.savefig(\"cford_m365_summary.png\", dpi=220, bbox_inches=\"tight\")\n",
    "print(\"Saved: cford_m365_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1757794362072,
     "user": {
      "displayName": "Crystal Ford",
      "userId": "09712735999583953291"
     },
     "user_tz": 240
    },
    "id": "3c1IiQmMtTK0",
    "outputId": "7f70cb7b-abaa-4e6b-9c06-047a0074a329"
   },
   "outputs": [],
   "source": [
    "plt.savefig(\"cford_m365_k_curve.png\", dpi=220, bbox_inches=\"tight\")\n",
    "print(\"Saved: cford_m365_k_curve.png\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTlXq87voH92XdaZKoS3Bb",
   "provenance": [
    {
     "file_id": "1IYrgS8MIQfFLIV6SGr3RfrLBB_hwYH7v",
     "timestamp": 1757794693390
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c2cf8d328f045399266cd6675c5a759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1d58727c4df0442bac579971fc1e8956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfa54d1777484733bf805d8007274080",
      "placeholder": "​",
      "style": "IPY_MODEL_65f77fc658b945338617096bdfef9c8e",
      "value": " 1/1 [00:01&lt;00:00,  1.18s/it]"
     }
    },
    "20222d2c025f4cfaa1f852561d06b3cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa0e58f748744787af9d39b03fd7d068",
      "placeholder": "​",
      "style": "IPY_MODEL_61f362063d6b4fc6ac3e7bc25e487bfd",
      "value": "Batches: 100%"
     }
    },
    "2ed65962b89b4d5ab47414273794c50d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfc08d1987b14958bf093d6efa9fa5ed",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c2cf8d328f045399266cd6675c5a759",
      "value": 1
     }
    },
    "61f362063d6b4fc6ac3e7bc25e487bfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65f77fc658b945338617096bdfef9c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa0e58f748744787af9d39b03fd7d068": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa2d7f8dfd0a40bcbcdda50f7ad6c45e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfc08d1987b14958bf093d6efa9fa5ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9749118625c4f58a844a585774bcd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20222d2c025f4cfaa1f852561d06b3cc",
       "IPY_MODEL_2ed65962b89b4d5ab47414273794c50d",
       "IPY_MODEL_1d58727c4df0442bac579971fc1e8956"
      ],
      "layout": "IPY_MODEL_aa2d7f8dfd0a40bcbcdda50f7ad6c45e"
     }
    },
    "dfa54d1777484733bf805d8007274080": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
